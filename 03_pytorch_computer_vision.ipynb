{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PyTorch Computer Vision\n",
        "\n"
      ],
      "metadata": {
        "id": "8_bFX4_5SSFK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Computer vision libraries in PyTorch\n",
        "\n",
        "* `torchvision` - base domain library for PyTorch computer vision\n",
        "* `torchvision.datasets` - get datasets and data loading functions for computer vision here\n",
        "* `torchvision.models` - get pretrained computer vision models that you can leverage for yoy own problems\n",
        "* `torchvision.transforms` - functions for manipulating your vision data (images) to be suitable for use with an ML model\n",
        "* `torch.utils.data.Dataset` - Base dataset class for PyTorch.\n",
        "* `torch.utils.data.DataLoader` - Creates a Python iterable over a dataset"
      ],
      "metadata": {
        "id": "ShyVYP44TwtK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import PyTorch\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# Import torchvision\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "# Import matplotlib for visualization\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Check versions\n",
        "print(torch.__version__)\n",
        "print(torchvision.__version__)"
      ],
      "metadata": {
        "id": "fwuDYl27T4w9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Getting a dataset\n",
        "\n",
        "The dataset we'll be using is FashionMNIST from torchvision.datasets"
      ],
      "metadata": {
        "id": "yLgcRFlpVbX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup training data\n",
        "train_data = datasets.FashionMNIST(\n",
        "    root=\"data\", # where to download data to\n",
        "    train=True, # do we want the training dataset?\n",
        "    download=True, # do we want to download?\n",
        "    transform=ToTensor(), # how do we want to transform the data?\n",
        "    target_transform=None # how do we want to transform the labels/target\n",
        ")\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        "    target_transform=None\n",
        ")"
      ],
      "metadata": {
        "id": "EGo-vn7QfZOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_data), len(test_data)"
      ],
      "metadata": {
        "id": "UO3mSqXXhGJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# See the first training example\n",
        "image, label = train_data[0]\n",
        "image, label"
      ],
      "metadata": {
        "id": "04soiMbWhGMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = train_data.classes\n",
        "class_names"
      ],
      "metadata": {
        "id": "1xM8oTVyhGRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_to_idx = train_data.class_to_idx\n",
        "class_to_idx"
      ],
      "metadata": {
        "id": "B0ECF9JwhGWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.targets"
      ],
      "metadata": {
        "id": "S-zKNFNhjf53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the shape of our image\n",
        "print(f\"Image shape: {image.shape} -> [color_channels, height, width]\")\n",
        "print(f\"Image label: {class_names[label]}\")"
      ],
      "metadata": {
        "id": "QRuby6W1jsN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Visualizing our data"
      ],
      "metadata": {
        "id": "t3c8JcxGL_hU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "image, label = train_data[0]\n",
        "print(f\"Image shape: {image.shape}\")\n",
        "plt.imshow(image.squeeze(), cmap=\"gray\")\n",
        "plt.title(class_names[label])\n",
        "plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "XFAxw3BrMd9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot more images\n",
        "torch.manual_seed(42)\n",
        "fig = plt.figure(figsize=(11, 11))\n",
        "rows, cols = 4, 4\n",
        "for i in range(1, rows*cols+1):\n",
        "  random_idx = torch.randint(0, len(train_data), size=[1]).item()\n",
        "  image, label = train_data[random_idx]\n",
        "  fig.add_subplot(rows, cols, i)\n",
        "  plt.imshow(image.squeeze(), cmap=\"gray\")\n",
        "  plt.title(class_names[label])\n",
        "  plt.axis(False)"
      ],
      "metadata": {
        "id": "EEfthVgWMOfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do you think these items of clothing (images) could be modelled with pure linear lines? Or do you think we'll need non-linearities?"
      ],
      "metadata": {
        "id": "B6zzG0azNRVl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Prepare DataLoader\n",
        "\n",
        "Right now, our data is in the form of PyTorch Datasets.\n",
        "\n",
        "DataLoader  turns our dataset into a Python iterable.\n",
        "\n",
        "More specifically, we want to turn our data into batches (or mini-batches).\n",
        "\n",
        "Whhy would we do this?\n",
        "\n",
        "1. It is more computionally efficent, as in, your computing hardware may to be able to look (store in memory) at 60000 images in one hit. So we break it down to 32 images at a time (batch size of 32).\n",
        "2. It gives our neural network more chances to update its gradients per epoch.\n"
      ],
      "metadata": {
        "id": "i1xbHSvbOZjZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "# Setup the batch size hyperparameter\n",
        "BATCH_SIZE = 32\n",
        "# turn datasets into iterables (batches)\n",
        "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)"
      ],
      "metadata": {
        "id": "seqdMVmOOo84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader, test_dataloader"
      ],
      "metadata": {
        "id": "1NQPg-z6OYH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's check out what what we've created\n",
        "print(f\"DataLoaders: {train_dataloader, test_dataloader}\")\n",
        "print(f\"Length of train_dataloader {len(train_dataloader)}, batches of {BATCH_SIZE}\")\n",
        "print(f\"Length of test_dataloader {len(test_dataloader)}, batches of {BATCH_SIZE}\")"
      ],
      "metadata": {
        "id": "bViFCs5qTW2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check out what's inside the training dataloader\n",
        "train_features_batch, train_labels_batch = next(iter(train_dataloader))\n",
        "train_features_batch.shape, train_labels_batch.shape"
      ],
      "metadata": {
        "id": "ASNI9GK8UpkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show a sample\n",
        "torch.manual_seed(42)\n",
        "random_idx = torch.randint(0, len(train_features_batch), size=[1]).item()\n",
        "image, label = train_features_batch[random_idx], train_labels_batch[random_idx]\n",
        "plt.imshow(image.squeeze(), cmap=\"gray\")\n",
        "plt.title(class_names[label])\n",
        "plt.axis(False)\n",
        "print(f\"Image size: {image.shape}\")\n",
        "print(f\"Label: {label}, label size {label.shape}\")"
      ],
      "metadata": {
        "id": "RBXWTEU0T87m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Model 0: Build a baseline model\n",
        "\n",
        "When starting to build a series of machine learning modelling experiments, it's best practice to start with a baseline model.\n",
        "\n",
        "A baseline model is a simple model you will try and improve upon with subsequent models/experiments.\n",
        "\n",
        "In other words: start simply and add complexity when necessary."
      ],
      "metadata": {
        "id": "cG1k6I_dT_vR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a flatten layer\n",
        "flatten_model = nn.Flatten()\n",
        "\n",
        "# Get a single sample\n",
        "x = train_features_batch[0]\n",
        "\n",
        "# Flatten the sample\n",
        "output = flatten_model(x) # perform forward pass\n",
        "\n",
        "# print out what happened\n",
        "print(f\"Shape before flattening: {x.shape}\") # color chanels, height, width\n",
        "print(f\"Shape after flattening: {output.shape}\") # color channels, height * width"
      ],
      "metadata": {
        "id": "mqnltA7pWaCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "class FashionMNISTModelV0(nn.Module):\n",
        "  def __init__(self,\n",
        "               input_shape:int,\n",
        "               hidden_units: int,\n",
        "               output_shape: int):\n",
        "    super().__init__()\n",
        "    self.layer_stack = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(in_features=input_shape,\n",
        "                  out_features=hidden_units),\n",
        "        nn.Linear(in_features=hidden_units,\n",
        "                  out_features=output_shape)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layer_stack(x)"
      ],
      "metadata": {
        "id": "6S1AOvHMX4Ub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "# setup model with input parameters\n",
        "model_0 = FashionMNISTModelV0(\n",
        "    input_shape=28*28,\n",
        "    hidden_units=10, # how many units in the hidden layer\n",
        "    output_shape=len(class_names)).to(\"cpu\") # one for every class\n",
        "\n",
        "model_0"
      ],
      "metadata": {
        "id": "JXz796zkaHOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_x = torch.rand([1, 1, 28, 28])\n",
        "model_0(dummy_x)"
      ],
      "metadata": {
        "id": "w43zOriuaumt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_0.state_dict()"
      ],
      "metadata": {
        "id": "ESRpGvyja3I2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Setup loss, optimizer and evaluation metrics\n",
        "\n",
        "* Loss function - since we're working with multi-class data, our loss function will be `nn.CrossEntropyLoss()`\n",
        "* Optimizer - our optimizr `torch.optim.SGD()` (stothastic gradient descent)\n",
        "* Evaluation metric - since we're working on a classification problem, let's use accuracy as our evaluation metric\n"
      ],
      "metadata": {
        "id": "V9Bz2Ny_lVHg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from pathlib import Path\n",
        "\n",
        "# Download helper functions from Learn PyTorch rep\n",
        "if Path(\"helper_functions.py\").is_file():\n",
        "  print(\"helper_functions.py already exists, skipping download\")\n",
        "else:\n",
        "  print(\"Downloading helper_functions.py\")\n",
        "  requests = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n",
        "  with open(\"helper_functions.py\", \"wb\") as f:\n",
        "    f.write(requests.content)\n"
      ],
      "metadata": {
        "id": "c8sNr5PKlobh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import accuracy metric\n",
        "from helper_functions import accuracy_fn\n",
        "\n",
        "# Setup loss function and optimizer\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(params=model_0.parameters(),\n",
        "                            lr=0.1)\n"
      ],
      "metadata": {
        "id": "syZxYh7dlohg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Creating a function to time our experiments\n",
        "\n",
        "Machine learning is very experimental.\n",
        "\n",
        "Two of the main things you'll often want to track are:\n",
        "1. Model's performance (loss and accuracy values etc)\n",
        "2. How fast it runs"
      ],
      "metadata": {
        "id": "HIjoqdEBln4S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from timeit import default_timer as timer\n",
        "def print_train_time(start: float,\n",
        "                     end: float,\n",
        "                     device: torch.device = None):\n",
        "  \"\"\"Print difference between start and end time.\"\"\"\n",
        "  total_time = end - start\n",
        "  print(f\"Train time on {device}: {total_time:.3f} seconds\")\n",
        "  return total_time"
      ],
      "metadata": {
        "id": "B3hgChnpoF46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = timer()\n",
        "# some code...\n",
        "end_time = timer()\n",
        "print_train_time(start=start_time, end=end_time, device=\"cpu\")"
      ],
      "metadata": {
        "id": "aqT2ng5wnyB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 Creating a training loop and training a model on batches of data...\n",
        "\n",
        "1. Loop through epochs.\n",
        "2. Loop through training batches, perform training steps, calculate the train loss *per batch*.\n",
        "3. Loop through testing batches, perform testing steps, calculate the test loss *per batch*.\n",
        "4. Print our what's happening.\n",
        "5. Time it all (for run)."
      ],
      "metadata": {
        "id": "sPIPKYgjo-m-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import tqdm for progress bar\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Set the seed and start the timer\n",
        "torch.manual_seed(42)\n",
        "train_time_start_on_cpu = timer()\n",
        "\n",
        "# Set the number of epochs (we'll keep this small for faster training time)\n",
        "epochs = 3\n",
        "\n",
        "# Create training and test loop\n",
        "for epoch in tqdm(range(epochs)):\n",
        "  print(f\"Epochs: {epochs}\\n------\")\n",
        "  ### training\n",
        "  train_loss = 0\n",
        "  # Add a loop to loop through the training batches\n",
        "  for batch, (X, y) in enumerate(train_dataloader):\n",
        "    model_0.train()\n",
        "    # 1. Forward pass\n",
        "    y_pred = model_0(X)\n",
        "\n",
        "    # 2. Calculate loss (per batch)\n",
        "    loss = loss_fn(y_pred, y)\n",
        "    train_loss += loss # accumulatively add up the loss per epoch\n",
        "\n",
        "    # 3. Optimizer zero grad\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # 4. Loss backward\n",
        "    loss.backward()\n",
        "\n",
        "    # 5. optimizer step\n",
        "    optimizer.step()\n",
        "\n",
        "    # print out what's happening\n",
        "    if batch % 400 == 0:\n",
        "      print(f\"Looked at {batch * len(X)}/{len(train_dataloader.dataset)} samples.\")\n",
        "\n",
        "  # Divide total train loss by length of train dataloader\n",
        "  train_loss /= len(train_dataloader)\n",
        "\n",
        "  ### Testing\n",
        "  test_loss, test_acc = 0, 0\n",
        "  model_0.eval()\n",
        "  with torch.inference_mode():\n",
        "    for X_test, y_test in test_dataloader:\n",
        "      # 1. forward pass\n",
        "      test_pred = model_0(X_test)\n",
        "\n",
        "      # 2. calculate loss\n",
        "      test_loss += loss_fn(test_pred, y_test)\n",
        "\n",
        "      # 3. calculate accuracy\n",
        "      test_acc += accuracy_fn(y_true=y_test,\n",
        "                               y_pred=test_pred.argmax(dim=1))\n",
        "    # calculate the test loss average per batch\n",
        "    test_loss /= len(test_dataloader)\n",
        "\n",
        "    # calculate the test accuracy average per batch\n",
        "    test_acc /= len(test_dataloader)\n",
        "\n",
        "  # print out what's happening\n",
        "  print(f\"\\nTrain loss: {train_loss:.4f}, Test loss: {test_loss:.4f}, Test acc: {test_acc:.2f}\")\n",
        "\n",
        "# calculate training time\n",
        "train_time_end_on_cpu = timer()\n",
        "total_train_time_model_0 = print_train_time(start=train_time_start_on_cpu,\n",
        "                                           end=train_time_end_on_cpu,\n",
        "                                            device=str(next(model_0.parameters()).device))\n"
      ],
      "metadata": {
        "id": "a3HYHjTZrdZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Make predictions and get Model 0 results"
      ],
      "metadata": {
        "id": "C96a-rR2rdiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "N_NL7_2ScVQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "def eval_model(model: torch.nn.Module,\n",
        "               data_loader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               accuracy_fn,\n",
        "               device=device):\n",
        "  \"\"\"Returns a dictionary containing the results of model predicting on data_loader.\"\"\"\n",
        "  loss, acc = 0, 0\n",
        "  model_0.eval()\n",
        "  with torch.inference_mode():\n",
        "    for X, y in data_loader:\n",
        "      # make predictions\n",
        "      X, y = X.to(device), y.to(device)\n",
        "      y_pred = model(X)\n",
        "      # Accumulate the loss and acc values per batch\n",
        "      loss += loss_fn(y_pred, y)\n",
        "      acc += accuracy_fn(y_true=y,\n",
        "                         y_pred=y_pred.argmax(dim=1))\n",
        "    # scale loss and acc to find the average loss/acc per batch\n",
        "    loss /= len(data_loader)\n",
        "    acc /= len(data_loader)\n",
        "\n",
        "  return {\"model_name\": model.__class__.__name__,  #only works when model was created with a class\n",
        "          \"model_loss\": loss.item(),\n",
        "          \"model_acc\": acc}\n",
        "model_0.to(device)\n",
        "# calculate model 0 results on test dataset\n",
        "model_0_results = eval_model(model=model_0,\n",
        "                             data_loader=test_dataloader,\n",
        "                             loss_fn=loss_fn,\n",
        "                             accuracy_fn=accuracy_fn)\n",
        "model_0_results"
      ],
      "metadata": {
        "id": "MIJWyXS9wt98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Setup device agnostic-code (for using a GPU if there is one)"
      ],
      "metadata": {
        "id": "Cs6yJKXZyjkQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "R8r4jWpLzFNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup device-agnostic code\n",
        "import torch\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "id": "HHPjjc-hzOdC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Model 1: Building a better model with non-linearity\n"
      ],
      "metadata": {
        "id": "aR52sagMzwkF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a model with non-linear and linear layers\n",
        "\n",
        "class FashionMNISTModelV1(nn.Module):\n",
        "  def __init__(self,\n",
        "              input_shape:int,\n",
        "              hidden_units:int,\n",
        "              output_shape:int):\n",
        "    super().__init__()\n",
        "    self.layer_stack = nn.Sequential(\n",
        "        nn.Flatten(), # flatten inputs into a single vector\n",
        "        nn.Linear(in_features=input_shape, out_features=hidden_units),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(in_features=hidden_units, out_features=output_shape),\n",
        "        nn.ReLU()\n",
        "        )\n",
        "\n",
        "\n",
        "  def forward(self, x: torch.Tensor):\n",
        "    return self.layer_stack(x)\n"
      ],
      "metadata": {
        "id": "uKVn8Mk4PBbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "model_1 = FashionMNISTModelV1(input_shape=28*28,\n",
        "                              hidden_units=10,\n",
        "                              output_shape=len(class_names)).to(device)"
      ],
      "metadata": {
        "id": "Gj1WkwIdPBqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.1 setup loss optimizer and evaluation metrics"
      ],
      "metadata": {
        "id": "e1lF5IiNQ2-e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from helper_functions import accuracy_fn\n",
        "# create a loss and optimizer\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(params=model_1.parameters(),\n",
        "                            lr=0.1)"
      ],
      "metadata": {
        "id": "_j0kUikoQZYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.2 Functionizing training and evaluation/testing loops\n",
        "\n",
        "Let's create a function for:\n",
        "* training loop- `train_step()`\n",
        "* testing loop- `test_step()`"
      ],
      "metadata": {
        "id": "1xTFuUn8Qryj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(model: torch.nn.Module,\n",
        "               data_loader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               optimizer: torch.optim.Optimizer,\n",
        "               accuracy_fn,\n",
        "               device: torch.device = device):\n",
        "  \"\"\"Performs a training with model trying to learn on data_loader\"\"\"\n",
        "  train_loss, train_acc = 0, 0\n",
        "\n",
        "  model.train()\n",
        "  for batch, (X, y) in enumerate(data_loader):\n",
        "    # put data on target device\n",
        "    X, y = X.to(device), y.to(device)\n",
        "    # 1. forward pass\n",
        "    y_pred = model(X)\n",
        "    # 2. calculate loss\n",
        "    loss = loss_fn(y_pred, y)\n",
        "    train_loss += loss\n",
        "    train_acc += accuracy_fn(y_true=y,\n",
        "                             y_pred=y_pred.argmax(dim=1)) # go from logits -> prediction labels\n",
        "\n",
        "    # 3. optimizer zero grad\n",
        "    optimizer.zero_grad()\n",
        "    # 4. loss backward\n",
        "    loss.backward()\n",
        "    # 5. optimizer step\n",
        "    optimizer.step()\n",
        "\n",
        "    # divide total trainn loss and acc by length of train dataloader\n",
        "  train_loss /= len(data_loader)\n",
        "  train_acc /= len(data_loader)\n",
        "  print(f\"Train loss: {train_loss:.4f}, Train acc: {train_acc:.2f}%\")"
      ],
      "metadata": {
        "id": "GmmK18HlRLnD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_step(model: torch.nn.Module,\n",
        "              data_loader: torch.utils.data.DataLoader,\n",
        "              loss_fn: torch.nn.Module,\n",
        "              accuracy_fn,\n",
        "              device: torch.device = device):\n",
        "  \"\"\"Performs a testing with model trying to learn on data_loader\"\"\"\n",
        "  test_loss, test_acc = 0, 0\n",
        "\n",
        "  model.eval()\n",
        "  with torch.inference_mode():\n",
        "    for X_test, y_test in data_loader:\n",
        "      # send the data to the target device\n",
        "      X_test , y_test = X_test.to(device), y_test.to(device)\n",
        "\n",
        "      # 1. forward pass\n",
        "      test_pred = model(X_test)\n",
        "\n",
        "      # 2. calculate loss\n",
        "      test_loss += loss_fn(test_pred, y_test)\n",
        "      test_acc += accuracy_fn(y_true=y_test,\n",
        "                               y_pred=test_pred.argmax(dim=1))\n",
        "    # adjust metrics and print out\n",
        "    test_loss /= len(data_loader)\n",
        "    test_acc /= len(data_loader)\n",
        "    print(f\"Test loss: {test_loss:.4f}, Test acc: {test_acc:.2f}%\\n\")\n"
      ],
      "metadata": {
        "id": "syrIUb87TTc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "# measure time\n",
        "from timeit import default_timer as timer\n",
        "train_time_start_on_gpu = timer()\n",
        "\n",
        "# set epochs\n",
        "epochs = 10\n",
        "# create a optimization and evaluation loop using train_step() and test_step()\n",
        "for epoch in tqdm(range(epochs)):\n",
        "  print(f\"Epochs: {epoch}\\n------\")\n",
        "  train_step(model=model_1,\n",
        "             data_loader=train_dataloader,\n",
        "             loss_fn = loss_fn,\n",
        "             optimizer = optimizer,\n",
        "             accuracy_fn=accuracy_fn,\n",
        "             device=device)\n",
        "  test_step(model=model_1,\n",
        "            data_loader=test_dataloader,\n",
        "            loss_fn=loss_fn,\n",
        "            accuracy_fn=accuracy_fn,\n",
        "            device=device)\n",
        "\n",
        "train_time_end_on_gpu = timer()\n",
        "total_train_time_model_1 = print_train_time(start=train_time_start_on_gpu, end=train_time_end_on_gpu, device=device)\n"
      ],
      "metadata": {
        "id": "HQ-sdQxhUhF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_0_results"
      ],
      "metadata": {
        "id": "D1bxox39V3m0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get model_1 results dictionary\n",
        "model_1_results = eval_model(model=model_1,\n",
        "                             data_loader=test_dataloader,\n",
        "                             loss_fn=loss_fn,\n",
        "                             accuracy_fn=accuracy_fn,\n",
        "                             device=device)\n",
        "model_1_results"
      ],
      "metadata": {
        "id": "aoCjQjk_YQjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MgZdQUy-dG_7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}